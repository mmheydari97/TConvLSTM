# Duration-Aware ConvLSTM

A unified implementation of a **Duration-Aware Convolutional LSTM Cell** across **PyTorch**, **TensorFlow (Keras)**, and **JAX (Flax)** frameworks. This model integrates temporal duration information into the ConvLSTM cell update, making it suitable for spatiotemporal modeling tasks where variable-duration events play a significant role.

---

## ðŸ“Œ Motivation

In standard ConvLSTM cells, the time intervals between frames are assumed to be uniform. However, many real-world sequences (e.g., clinical time series, surveillance video, event-based vision) contain *non-uniform durations*. This module modifies the classic ConvLSTM to:

* Accept a scalar **duration input** at each timestep.
* Modulate the candidate cell state using a learned function of the duration.
* Allow *temporal irregularity* to influence state updates.

---

## ðŸ“¦ Features

* âœ… Implemented in **PyTorch**, **TensorFlow (Keras)**, and **JAX (Flax)**.
* ðŸ“ Flexible architecture for kernel size, hidden channels, and MLP structure.
* â±ï¸ Multiple strategies for modeling duration impact (linear, log, learned scalar).
* ðŸ§  Ready for integration with sequence models (e.g., `RNN`, `scan`, looped inference).

---

## ðŸ”§ Installation

To run the code in each framework:

### PyTorch

```bash
pip install torch
```

### TensorFlow

```bash
pip install tensorflow
```

### JAX (Flax or Haiku)

```bash
pip install flax optax jax jaxlib
```

> For GPU support in JAX, refer to: [https://github.com/google/jax#installation](https://github.com/google/jax#installation)

---

## ðŸ§  Architecture Overview

Each Duration-Aware ConvLSTM cell consists of:

* Standard ConvLSTM gates: input (i), forget (f), output (o), and candidate cell state (gÌƒ).
* A **duration-aware MLP** that transforms a scalar duration into a multiplicative factor applied to `gÌƒ`.
* The new cell state is computed as:

```
c_t = f âŠ™ c_{t-1} + i âŠ™ (gÌƒ Ã— duration_factor)
h_t = o âŠ™ tanh(c_t)
```

---

## ðŸ§ª Usage

### PyTorch Example

```python
rnn_cell = DurationAwareConvLSTMCell(input_dim=3, hidden_dim=64, kernel_size=(3,3))
h, c = rnn_cell.init_hidden(batch_size=4, image_size=(64, 64))

for frame, duration in zip(sequence_frames, sequence_durations):
    h, c = rnn_cell(frame, duration, (h, c))
```

### TensorFlow Example

```python
cell = DurationAwareConvLSTMCell(filters=64, kernel_size=(3,3))
rnn_layer = tf.keras.layers.RNN(cell, return_sequences=True)

output = rnn_layer((frames_input, durations_input))  # Sequence input
```

### JAX (Flax) Example

```python
cell = DurationAwareConvLSTMCell(hidden_features=64, kernel_size=(3,3))
initial_state = (h0, c0)

def scan_fn(carry, inputs):
    return cell().apply({'params': params}, carry, inputs)

final_state, outputs = jax.lax.scan(scan_fn, initial_state, (frames_seq, durations_seq))
```

---

## ðŸ”¬ Duration Scaling Strategies

| Strategy         | Description                                 |
| ---------------- | ------------------------------------------- |
| Learned MLP      | Transforms duration â†’ scaling vector/factor |
| Log-based        | Uses `log(duration + Îµ) + 1.0`              |
| Exponential Damp | Uses `exp(scalar Ã— 0.1)` to control growth  |

These strategies can be toggled or customized based on your task sensitivity to time variance.

---

## ðŸ§± Applications

* Clinical time-series forecasting (e.g., irregular vitals)
* Event-based video summarization
* Spatiotemporal forecasting with non-uniform frame rates
* Motion prediction in robotics with variable time steps

---

